{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2aff89f4-1975-4135-8dbd-708ea7e98692",
     "showTitle": true,
     "title": "Reporters"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+----------+-------+-------------+---+-----+\n| RID|First_Name| Last_Name|   City|Mobile_Number|Age|  EID|\n+----+----------+----------+-------+-------------+---+-----+\n|R102|    Jaimin|      Jani|Mehsana|   9898177712| 28|ER102|\n|R101|    Alpesh|     Patel|Bhadran|   9898177711| 30|ER101|\n|R103|   Yagnesh|Bramhbhatt| Rajkot|   9898177713| 35|ER103|\n+----+----------+----------+-------+-------------+---+-----+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, LongType\n",
    "\n",
    "\"\"\"\n",
    "# Step 1: Import the necessary libraries\n",
    "\n",
    "# Create Database \"News\"\n",
    "# spark.sql(\"create database News comment 'This database created using python'\")\n",
    "\n",
    "# Step 2: Initialize the Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Create Table in Databricks\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Step 3: Define the schema of the table\n",
    "schema = StructType([\n",
    "    StructField(\"RID\", StringType(), True),\n",
    "    StructField(\"First_Name\", StringType(), True),\n",
    "    StructField(\"Last_Name\", StringType(), True),\n",
    "    StructField(\"City\", StringType(), True),\n",
    "    StructField(\"Mobile_Number\", LongType(), True),\n",
    "    StructField(\"Age\", IntegerType(), True),\n",
    "    StructField(\"EID\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "# Step 4: Create the DataFrame\n",
    "data = [\n",
    "    (\"R101\", \"Alpesh\", \"Patel\", \"Bhadran\", 9898177711, 30, E101),\n",
    "    (\"R102\", \"Jaimin\", \"Jani\", \"Mehsana\", 9898177712, 28, E102),\n",
    "    (\"R103\", \"Yagnesh\", \"Bramhbhatt\", \"Rajot\", 9898177713, 35, E103)\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(data, schema)\n",
    "\n",
    "# Step 5: Save the DataFrame as a table in the \"News\" database\n",
    "database_name = \"News\"\n",
    "table_name = \"Reporters\" \n",
    "\n",
    "# Use the format \"database_name.table_name\" to specify the database while saving the table\n",
    "df.write.saveAsTable(f\"{database_name}.{table_name}\")\n",
    "\n",
    "# Step 6: Show the data from the table\n",
    "df_from_table = spark.table(f\"{database_name}.{table_name}\")\n",
    "df_from_table.show() \"\"\"\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize the Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Print Reporters Table\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Set the database name where the table exists\n",
    "database_name = \"News\"\n",
    "table_name = \"Reporters\"\n",
    "\n",
    "# Read the \"Reporters\" table into a DataFrame\n",
    "reporters_df = spark.table(f\"{database_name}.{table_name}\")\n",
    "\n",
    "# Print the contents of the \"Reporters\" table\n",
    "reporters_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "82efb928-44ab-4c10-916a-794478091765",
     "showTitle": true,
     "title": "Cameraman"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+---------+---------+-------------+---+----+\n| CID|First_Name|Last_Name|     City|Mobile_Number|Age| EID|\n+----+----------+---------+---------+-------------+---+----+\n|C101|  Prashant|   Pandya| Vadodara|   9899177711| 24|E104|\n|C103|     Bakul|     Buch|Bhavnagar|   9899177713| 30|E106|\n|C102|     Vinay|    Mehta| Junagadh|   9899177712| 26|E105|\n+----+----------+---------+---------+-------------+---+----+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, LongType\n",
    "\n",
    "\"\"\"\n",
    "# Step 1: Initialize the Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Create Table with Values in Databricks\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Step 2: Define the schema of the table\n",
    "schema = StructType([\n",
    "    StructField(\"CID\", StringType(), True),\n",
    "    StructField(\"First_Name\", StringType(), True),\n",
    "    StructField(\"Last_Name\", StringType(), True),\n",
    "    StructField(\"City\", StringType(), True),\n",
    "    StructField(\"Mobile_Number\", LongType(), True),\n",
    "    StructField(\"Age\", IntegerType(), True),\n",
    "    StructField(\"EID\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "# Step 3: Create the DataFrame with values\n",
    "data = [\n",
    "    (\"C101\", \"Prashant\", \"Pandya\", \"Vadodara\", 9899177711, 24),\n",
    "    (\"C102\", \"Vinay\", \"Mehta\", \"Junagadh\", 9899177712, 26),\n",
    "    (\"C103\", \"Bakul\", \"Buch\", \"Bhavnagar\", 9899177713, 30)\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(data, schema)\n",
    "\n",
    "# Step 4: Save the DataFrame as a table in the \"News\" database\n",
    "database_name = \"News\"\n",
    "table_name = \"Cameraman\"\n",
    "\n",
    "# Use the format \"database_name.table_name\" to specify the database while saving the table\n",
    "df.write.saveAsTable(f\"{database_name}.{table_name}\")\n",
    "\n",
    "# Step 5: Show the data from the table\n",
    "df_from_table = spark.table(f\"{database_name}.{table_name}\")\n",
    "df_from_table.show() \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5e3ed7e4-10b6-450d-8a47-84e79910df6b",
     "showTitle": true,
     "title": "Employee ID"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+----------+--------------------+---------+-------------+-----------------+----------+---+------+\n| EID|First_Name| Last_Name|             Address|     City|Mobile_Number|          Manager|Department|Age|Salary|\n+----+----------+----------+--------------------+---------+-------------+-----------------+----------+---+------+\n|E104|  Prashant|    Pandya|         Mama Ni Pol| Vadodara|   9899177711|Ramakant Upadhyay| Cameraman| 24| 25000|\n|E102|    Jaimin|      Jani|SimandharSwami So...|  Mehsana|   9898177712|     Harish Bhatt|  Reporter| 28| 32000|\n|E103|   Yagnesh|Bramhbhatt|        Bapu Ni Vadi|    Rajot|   9898177713|    Vijay Goswami|  Reporter| 35| 36000|\n|E105|     Vinay|     Mehta|           Joshipara| Junagadh|   9899177712|    Jagdish Raval| Cameraman| 26| 28000|\n|E106|     Bakul|      Buch|      Ambawadi Chowk|Bhavnagar|   9899177713| Yuvrajsinh Gohil| Cameraman| 30| 30000|\n|E101|    Alpesh|     Patel|           DadaNagar|  Bhadran|   9898177711|    Mansukh Patel|  Reporter| 30| 35000|\n+----+----------+----------+--------------------+---------+-------------+-----------------+----------+---+------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, LongType\n",
    "\n",
    "\"\"\"\n",
    "# Step 1: Initialize the Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Create Table with Values in Databricks\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Step 2: Define the schema of the table\n",
    "schema = StructType([\n",
    "    StructField(\"EID\", StringType(), True),\n",
    "    StructField(\"First_Name\", StringType(), True),\n",
    "    StructField(\"Last_Name\", StringType(), True),\n",
    "    StructField(\"Address\", StringType(), True),\n",
    "    StructField(\"City\", StringType(), True),\n",
    "    StructField(\"Mobile_Number\", LongType(), True),\n",
    "    StructField(\"Manager\", StringType(), True),\n",
    "    StructField(\"Department\", StringType(), True),                 \n",
    "    StructField(\"Age\", IntegerType(), True),\n",
    "    StructField(\"Salary\", LongType(), True)\n",
    "])\n",
    "\n",
    "# Step 3: Create the DataFrame with values\n",
    "data = [\n",
    "    (\"E101\", \"Alpesh\", \"Patel\", \"DadaNagar\",\"Bhadran\", 9898177711, \"Mansukh Patel\", \"Reporter\", 30, 35000),\n",
    "    (\"E102\", \"Jaimin\", \"Jani\", \"SimandharSwami Society\",\"Mehsana\", 9898177712, \"Harish Bhatt\", \"Reporter\", 28, 32000),\n",
    "    (\"E103\", \"Yagnesh\", \"Bramhbhatt\", \"Bapu Ni Vadi\", \"Rajot\", 9898177713, \"Vijay Goswami\", \"Reporter\", 35, 36000),\n",
    "    (\"E104\", \"Prashant\", \"Pandya\", \"Mama Ni Pol\", \"Vadodara\", 9899177711, \"Ramakant Upadhyay\", \"Cameraman\", 24, 25000),\n",
    "    (\"E105\", \"Vinay\", \"Mehta\", \"Joshipara\", \"Junagadh\", 9899177712, \"Jagdish Raval\", \"Cameraman\", 26, 28000),\n",
    "    (\"E106\", \"Bakul\", \"Buch\", \"Ambawadi Chowk\",\"Bhavnagar\", 9899177713, \"Yuvrajsinh Gohil\", \"Cameraman\", 30, 30000)\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(data, schema)\n",
    "\n",
    "# Step 4: Save the DataFrame as a table in the \"News\" database\n",
    "database_name = \"News\"\n",
    "table_name = \"Employee\"\n",
    "\n",
    "# Use the format \"database_name.table_name\" to specify the database while saving the table\n",
    "df.write.saveAsTable(f\"{database_name}.{table_name}\")\n",
    "\n",
    "# Step 5: Show the data from the table\n",
    "df_from_table = spark.table(f\"{database_name}.{table_name}\")\n",
    "df_from_table.show() \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1fd70d34-1ba0-4520-aaa0-1c347c795c25",
     "showTitle": true,
     "title": "Joins"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+----------+--------------------+-------+-------------+-------------+----------+---+------+----+----------+----------+-------+-------------+---+\n| EID|First_Name| Last_Name|             Address|   City|Mobile_Number|      Manager|Department|Age|Salary| RID|First_Name| Last_Name|   City|Mobile_Number|Age|\n+----+----------+----------+--------------------+-------+-------------+-------------+----------+---+------+----+----------+----------+-------+-------------+---+\n|E102|    Jaimin|      Jani|SimandharSwami So...|Mehsana|   9898177712| Harish Bhatt|  Reporter| 28| 32000|R102|    Jaimin|      Jani|Mehsana|   9898177712| 28|\n|E101|    Alpesh|     Patel|           DadaNagar|Bhadran|   9898177711|Mansukh Patel|  Reporter| 30| 35000|R101|    Alpesh|     Patel|Bhadran|   9898177711| 30|\n|E103|   Yagnesh|Bramhbhatt|        Bapu Ni Vadi|  Rajot|   9898177713|Vijay Goswami|  Reporter| 35| 36000|R103|   Yagnesh|Bramhbhatt| Rajkot|   9898177713| 35|\n+----+----------+----------+--------------------+-------+-------------+-------------+----------+---+------+----+----------+----------+-------+-------------+---+\n\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Initialize the Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Joining Employee and Reporters DataFrames\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Step 2: Create the Employee DataFrame with values\n",
    "# (Assuming you have already created the Employee DataFrame as shown in your previous code)\n",
    "\n",
    "# Step 3: Create the Reporters DataFrame with values\n",
    "# (Assuming you have already created the Reporters DataFrame as shown in your previous code)\n",
    "\n",
    "# Step 4: Perform an inner join on the \"EID\" column\n",
    "result_df = employee_df.join(reporters_df, \"EID\", \"inner\")\n",
    "\n",
    "# Step 5: Show the result of the inner join\n",
    "result_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8e473001-7bf0-4eab-87d6-626b6ab823ef",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+----------+----------------------+---------+-------------+-----------------+----------+---+------+---------+\n|EID |First_Name|Last_Name |Address               |City     |Mobile_Number|Manager          |Department|Age|Salary|Employee |\n+----+----------+----------+----------------------+---------+-------------+-----------------+----------+---+------+---------+\n|E104|Prashant  |Pandya    |Mama Ni Pol           |Vadodara |9899177711   |Ramakant Upadhyay|Cameraman |24 |25000 |Vadodara |\n|E102|Jaimin    |Jani      |SimandharSwami Society|Mehsana  |9898177712   |Harish Bhatt     |Reporter  |28 |32000 |Mehsana  |\n|E103|Yagnesh   |Bramhbhatt|Bapu Ni Vadi          |Rajot    |9898177713   |Vijay Goswami    |Reporter  |35 |36000 |Rajkot   |\n|E105|Vinay     |Mehta     |Joshipara             |Junagadh |9899177712   |Jagdish Raval    |Cameraman |26 |28000 |Junagadh |\n|E106|Bakul     |Buch      |Ambawadi Chowk        |Bhavnagar|9899177713   |Yuvrajsinh Gohil |Cameraman |30 |30000 |Bhavnagar|\n|E101|Alpesh    |Patel     |DadaNagar             |Bhadran  |9898177711   |Mansukh Patel    |Reporter  |30 |35000 |Bhadran  |\n+----+----------+----------+----------------------+---------+-------------+-----------------+----------+---+------+---------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, when\n",
    "from pyspark.sql.functions import regexp_replace\n",
    "\n",
    "df.withColumn('Employee', regexp_replace('City', 'Rajot', 'Rajkot')) \\\n",
    "  .show(truncate=False)\n",
    "\n",
    "# Step 5: Show the updated Employee DataFrame\n",
    "# employee_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "030a7694-275b-497c-840d-84445f3bb1c9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+----------+----------------------+---------+-------------+-----------------+----------+---+------+\n|EID |First_Name|Last_Name |Address               |City     |Mobile_Number|Manager          |Department|Age|Salary|\n+----+----------+----------+----------------------+---------+-------------+-----------------+----------+---+------+\n|E104|Prashant  |Pandya    |Mama Ni Pol           |Vadodara |9899177711   |Ramakant Upadhyay|Cameraman |24 |25000 |\n|E102|Jaimin    |Jani      |SimandharSwami Society|Mehsana  |9898177712   |Harish Bhatt     |Reporter  |28 |32000 |\n|E103|Yagnesh   |Bramhbhatt|Bapu Ni Vadi          |Rajkot   |9898177713   |Vijay Goswami    |Reporter  |35 |36000 |\n|E105|Vinay     |Mehta     |Joshipara             |Junagadh |9899177712   |Jagdish Raval    |Cameraman |26 |28000 |\n|E106|Bakul     |Buch      |Ambawadi Chowk        |Bhavnagar|9899177713   |Yuvrajsinh Gohil |Cameraman |30 |30000 |\n|E101|Alpesh    |Patel     |DadaNagar             |Bhadran  |9898177711   |Mansukh Patel    |Reporter  |30 |35000 |\n+----+----------+----------+----------------------+---------+-------------+-----------------+----------+---+------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import when, regexp_replace\n",
    "\n",
    "# Assuming you have already loaded the DataFrame with the name 'df'\n",
    "# If not, you need to load it first using SparkSession or any other method\n",
    "\n",
    "# Update the 'City' column to replace \"Rajot\" with \"Rajkot\"\n",
    "updated_df = df.withColumn('City', \n",
    "                           when(df.City.endswith('Rajot'), regexp_replace(df.City, 'Rajot', 'Rajkot'))\n",
    "                           .otherwise(df.City))\n",
    "\n",
    "# Show the updated DataFrame\n",
    "updated_df.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "de92909d-240e-4cdf-bf5e-a29104fb7b74",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+----------+----------------------+---------+-------------+-----------------+----------+---+------+\n|EID |First_Name|Last_Name |Address               |City     |Mobile_Number|Manager          |Department|Age|Salary|\n+----+----------+----------+----------------------+---------+-------------+-----------------+----------+---+------+\n|E104|Prashant  |Pandya    |Mama Ni Pol           |Vadodara |9899177711   |Ramakant Upadhyay|Cameraman |24 |25000 |\n|E102|Jaimin    |Jani      |SimandharSwami Society|Mehsana  |9898177712   |Harish Bhatt     |Reporter  |28 |32000 |\n|E103|Yagnesh   |Bramhbhatt|Bapu Ni Vadi          |Rajkot   |9898177713   |Vijay Goswami    |Reporter  |35 |36000 |\n|E105|Vinay     |Mehta     |Joshipara             |Junagadh |9899177712   |Jagdish Raval    |Cameraman |26 |28000 |\n|E106|Bakul     |Buch      |Ambawadi Chowk        |Bhavnagar|9899177713   |Yuvrajsinh Gohil |Cameraman |30 |30000 |\n|E101|Alpesh    |Patel     |DadaNagar             |Bhadran  |9898177711   |Mansukh Patel    |Reporter  |30 |35000 |\n+----+----------+----------+----------------------+---------+-------------+-----------------+----------+---+------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import when, regexp_replace\n",
    "\n",
    "# Step 1: Initialize the Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Changing City Name in Employee DataFrame\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Step 2: Load the DataFrame from the table in Databricks\n",
    "database_name = \"News\"\n",
    "employee_table_name = \"Employee\"\n",
    "df = spark.table(f\"{database_name}.{employee_table_name}\")\n",
    "\n",
    "# Step 3: Update the 'City' column to replace \"Rajot\" with \"Rajkot\"\n",
    "updated_df = df.withColumn('City', \n",
    "                           when(df.City.endswith('Rajot'), regexp_replace(df.City, 'Rajot', 'Rajkot'))\n",
    "                           .otherwise(df.City))\n",
    "\n",
    "# Step 4: Save the updated DataFrame as a table in Databricks, overwriting the existing table\n",
    "updated_df.write.mode(\"overwrite\").saveAsTable(f\"{database_name}.{employee_table_name}\")\n",
    "\n",
    "# Step 5: Show the updated DataFrame\n",
    "updated_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d85615a8-662c-416a-a5b2-31da4e1979c0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Inner Join - Matching Values from both the tables\n",
    "# Left Join - Matching Values of other tables are printed otherwise null is mentioned\n",
    "# Right Join - From Table as it is but value may differ"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "News",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}